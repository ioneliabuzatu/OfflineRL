{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRnY963jq1CW"
   },
   "source": [
    "Authors: Dinu, Hofmarcher\n",
    "\n",
    "Date: 17-03-2021\n",
    "\n",
    "---\n",
    "\n",
    "This file is part of the \"Deep Reinforcement Learning\" lecture material. The following copyright statement applies to all code within this file.\n",
    "\n",
    "Copyright statement:\n",
    "This material, no matter whether in printed or electronic form, may be used for personal and non-commercial educational use only. Any reproduction of this manuscript, no matter whether as a whole or in parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eryXuKadrmRK"
   },
   "source": [
    "## Enable GPU Acceleration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e5BB3G1hXL5"
   },
   "source": [
    "---\n",
    "Before you start exploring this notebook make sure that GPU support is enabled.\n",
    "To enable the GPU backend for your notebook, go to **Edit** â†’ **Notebook Settings** and set **Hardware accelerator** to **GPU**. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBUs4yMsgRSz"
   },
   "source": [
    "# Install required packages and import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D64rNsQCyL6Q"
   },
   "source": [
    "Install OpenAI Gym and dependencies to render the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqL6W_Gkgp9a"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!apt update\n",
    "!apt-get install -y xvfb x11-utils ffmpeg\n",
    "!pip install gym==0.17.3 pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
    "!pip install onnx onnx2pytorch\n",
    "# Install environments\n",
    "!pip install gym[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ECmcPAOnhR4"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# ==============[DEBUGGING SEED]==============\n",
    "# Don't forget to remove seeding and/or\n",
    "# test on multiple seeds.\n",
    "seed = None\n",
    "import random\n",
    "if seed: random.seed(seed)\n",
    "import numpy as np\n",
    "if seed: np.random.seed(seed)\n",
    "import torch\n",
    "if seed: torch.manual_seed(seed)\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import copy\n",
    "import zipfile\n",
    "from time import sleep\n",
    "\n",
    "# PyTorch imports\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "from torchvision.transforms import Compose, ToTensor, Grayscale, ToPILImage\n",
    "# Onnx model-export imports\n",
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "# Auxiliary Python imports\n",
    "import math\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from tqdm.notebook import tqdm\n",
    "from time import sleep, time, strftime\n",
    "from collections import namedtuple\n",
    "\n",
    "# Environment import and set logger level to display error only\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "gymlogger.set_level(40) # error only\n",
    "\n",
    "# Plotting and notebook imports\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "# Plotting and notebook imports\n",
    "from IPython.display import HTML, clear_output\n",
    "from IPython import display\n",
    "from ipywidgets import Output\n",
    "\n",
    "# start virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "pydisplay = Display(visible=0, size=(640, 480))\n",
    "pydisplay.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IuPNzGVgGR0"
   },
   "source": [
    "# Download Dataset and Unzip File\n",
    "\n",
    "In contrast to Imitation Learning, data in Offline RL is not always perfect and can contain good and bad samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmvfV_CjMSA6"
   },
   "source": [
    "## Mixed Data\n",
    "\n",
    "This data was collected by a PPO agent and logged during the training process. It contains expert as well as imperfect data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sy79qvPFfLRp"
   },
   "outputs": [],
   "source": [
    "!wget --no-check-certificate 'https://cloud.ml.jku.at/s/CdYdidkkBpFgcED/download' -O train_mixed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsUS_052Bm_t"
   },
   "outputs": [],
   "source": [
    "# select as a data root the mixed demonstratoins directory\n",
    "data_root = 'data_mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWFMHIN6znTe"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('train_mixed.zip', 'r') as zip_ref:\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "    zip_ref.extractall(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5chkQUwj4pT"
   },
   "source": [
    "# Auxiliary Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrjYL01ojsAD"
   },
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, logdir, params=None):\n",
    "        self.basepath = os.path.join(logdir, strftime(\"%Y-%m-%dT%H-%M-%S\"))\n",
    "        os.makedirs(self.basepath, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        if params is not None and os.path.exists(params):\n",
    "            shutil.copyfile(params, os.path.join(self.basepath, \"params.pkl\"))\n",
    "        self.log_dict = {}\n",
    "        self.dump_idx = {}\n",
    "\n",
    "    @property\n",
    "    def param_file(self):\n",
    "        return os.path.join(self.basepath, \"params.pkl\")\n",
    "\n",
    "    @property\n",
    "    def onnx_file(self):\n",
    "        return os.path.join(self.basepath, \"model.onnx\")\n",
    "\n",
    "    @property\n",
    "    def log_dir(self):\n",
    "        return os.path.join(self.basepath, \"logs\")\n",
    "\n",
    "    def log(self, name, value):\n",
    "        if name not in self.log_dict:\n",
    "            self.log_dict[name] = []\n",
    "            self.dump_idx[name] = -1\n",
    "        self.log_dict[name].append((len(self.log_dict[name]), time(), value))\n",
    "    \n",
    "    def get_values(self, name):\n",
    "        if name in self.log_dict:\n",
    "            return [x[2] for x in self.log_dict[name]]\n",
    "        return None\n",
    "    \n",
    "    def dump(self):\n",
    "        for name, rows in self.log_dict.items():\n",
    "            with open(os.path.join(self.log_dir, name + \".log\"), \"a\") as f:\n",
    "                for i, row in enumerate(rows):\n",
    "                    if i > self.dump_idx[name]:\n",
    "                        f.write(\",\".join([str(x) for x in row]) + \"\\n\")\n",
    "                        self.dump_idx[name] = i\n",
    "\n",
    "\n",
    "def plot_metrics(logger):\n",
    "    train_loss  = logger.get_values(\"training_loss\")\n",
    "    env_score  = logger.get_values(\"env_score\")\n",
    "    \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax1 = fig.add_subplot(131, label=\"train\")\n",
    "    ax2 = fig.add_subplot(132, label=\"score\")\n",
    "\n",
    "    ax1.plot(train_loss, color=\"C0\")\n",
    "    ax1.set_ylabel(\"Loss\", color=\"black\")\n",
    "    ax1.set_xlabel(\"Epoch\", color=\"black\")\n",
    "    ax1.tick_params(axis='x', colors=\"black\")\n",
    "    ax1.tick_params(axis='y', colors=\"black\")\n",
    "    ax1.set_ylim((0, 10))\n",
    "\n",
    "    ax2.plot(env_score, color=\"C1\")\n",
    "    ax2.set_ylabel(\"Score\", color=\"black\")\n",
    "    ax2.set_xlabel(\"Epoch\", color=\"black\")\n",
    "    ax2.tick_params(axis='x', colors=\"black\")\n",
    "    ax2.tick_params(axis='y', colors=\"black\")\n",
    "    ax2.set_ylim((-100, 1000))\n",
    "\n",
    "    fig.tight_layout(pad=2.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_action(action):\n",
    "    print(\"Left %.1f\" % action[0] if action[0] < 0 else \"Right %.1f\" % action[0] if action[0] > 0 else \"Straight\")\n",
    "    print(\"Throttle %.1f\" % action[1])\n",
    "    print(\"Break %.1f\" % action[2])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "\"\"\"\n",
    "def concatenate_videos(video_dir):\n",
    "    \"\"\"\n",
    "    Merge all mp4 videos in video_dir.\n",
    "    \"\"\"\n",
    "    outfile = os.path.join(video_dir, 'merged_video.mp4')\n",
    "    cmd = \"ffmpeg -i \\\"concat:\"\n",
    "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
    "    tmpfiles = []\n",
    "    # build ffmpeg command and create temp files\n",
    "    for f in mp4list:\n",
    "        file = os.path.join(video_dir, \"temp\" + str(mp4list.index(f) + 1) + \".ts\")\n",
    "        os.system(\"ffmpeg -i \" + f + \" -c copy -bsf:v h264_mp4toannexb -f mpegts \" + file)\n",
    "        tmpfiles.append(file)\n",
    "    for f in tmpfiles:\n",
    "        cmd += f\n",
    "        if tmpfiles.index(f) != len(tmpfiles)-1:\n",
    "            cmd += \"|\"\n",
    "        else:\n",
    "            cmd += f\"\\\" -c copy  -bsf:a aac_adtstoasc {outfile}\"\n",
    "    # execute ffmpeg command to combine videos\n",
    "    os.system(cmd)\n",
    "    # cleanup\n",
    "    for f in tmpfiles + mp4list:\n",
    "        if f != outfile:\n",
    "            os.remove(f)\n",
    "    # --\n",
    "    return outfile\n",
    "\n",
    "\n",
    "def show_video(video_dir):\n",
    "    \"\"\"\n",
    "    Show video in the output of a code cell.\n",
    "    \"\"\"\n",
    "    # merge all videos\n",
    "    mp4 = concatenate_videos(video_dir)    \n",
    "    if mp4:\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "\n",
    "\n",
    "# Convert RBG image to grayscale and normalize by data statistics\n",
    "def rgb2gray(rgb, norm=True):\n",
    "    # rgb image -> gray [0, 1]\n",
    "    gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "    if norm:\n",
    "        # normalize\n",
    "        gray = gray / 128. - 1.\n",
    "    return gray\n",
    "\n",
    "\n",
    "def hide_hud(img):\n",
    "    img[84:] = 0\n",
    "    return img\n",
    "\n",
    "\n",
    "# Use to download colab parameter file\n",
    "def download_colab_model(param_file):\n",
    "    from google.colab import files\n",
    "    files.download(param_file)\n",
    "\n",
    "\n",
    "def save_as_onnx(torch_model, sample_input, model_path):\n",
    "    torch.onnx.export(torch_model,             # model being run\n",
    "                    sample_input,              # model input (or a tuple for multiple inputs)\n",
    "                    f=model_path,              # where to save the model (can be a file or file-like object)\n",
    "                    export_params=True,        # store the trained parameter weights inside the model file\n",
    "                    opset_version=13,          # the ONNX version to export the model to - see https://github.com/microsoft/onnxruntime/blob/master/docs/Versioning.md\n",
    "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3QTC6sgj2Eq"
   },
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9ihClXhjvOT"
   },
   "outputs": [],
   "source": [
    "# Action space (map from continuous actions for steering, throttle and break to 25 action combinations)\n",
    "action_mapping = [\n",
    "    (0, 0, 0),  # no action\n",
    "    (0, 0.5, 0),  # half throttle\n",
    "    (0, 1, 0),  # full trottle\n",
    "    (0, 0, 0.5),  # half break\n",
    "    (0, 0, 1),  # full break\n",
    "    # steering left with throttle/break control\n",
    "    (-0.5, 0, 0),  # half left\n",
    "    (-1, 0, 0),  # full left\n",
    "    (-0.5, 0.5, 0),  # half left\n",
    "    (-1, 0.5, 0),  # full left\n",
    "    (-0.5, 1, 0),  # half left\n",
    "    (-1, 1, 0),  # full left\n",
    "    (-0.5, 0, 0.5),  # half left\n",
    "    (-1, 0, 0.5),  # full left\n",
    "    (-0.5, 0, 1),  # half left\n",
    "    (-1, 0, 1),  # full left\n",
    "    # steering right with throttle/break control\n",
    "    (0.5, 0, 0),  # half right\n",
    "    (1, 0, 0),  # full right\n",
    "    (0.5, 0.5, 0),  # half right\n",
    "    (1, 0.5, 0),  # full right\n",
    "    (0.5, 1, 0),  # half right\n",
    "    (1, 1, 0),  # full right\n",
    "    (0.5, 0, 0.5),  # half right\n",
    "    (1, 0, 0.5),  # full right\n",
    "    (0.5, 0, 1),  # half right\n",
    "    (1, 0, 1)  # full right\n",
    "]\n",
    "\n",
    "# create transition object for partial demonstrations\n",
    "Transition = namedtuple('Transition', ['states', 'actions', 'next_states', 'rewards', 'dones'])\n",
    "\n",
    "# Since the demonstrations are partial files assuming that the collected data is too\n",
    "# large to fit into memory at once the Demonstration class utilizes an object \n",
    "# from the ParialDataset class to load and unload files from the file system.\n",
    "# This is a typical use case for very large datasets and should give you an idea \n",
    "# how to handle such issues.  \n",
    "class Demonstration(object):\n",
    "    def __init__(self, root_path):\n",
    "        assert (os.path.exists(root_path))\n",
    "        self.root_path = root_path\n",
    "        # assign list of data files found in the data root directory\n",
    "        self.data_files = sorted(os.listdir(root_path))\n",
    "\n",
    "    def __len__(self):\n",
    "        # this count returns the number of files in the data root folder\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def load(self, idx):\n",
    "        # select an index at random from all files\n",
    "        file_name = self.data_files[idx]\n",
    "        file_path = os.path.join(self.root_path, file_name)\n",
    "        # load the selected file\n",
    "        data = np.load(file_path)\n",
    "        # get the respective properties from the files\n",
    "        states = data[\"states\"]\n",
    "        actions = data[\"actions\"]\n",
    "        next_states = data[\"next_states\"]\n",
    "        rewards = data[\"rewards\"]\n",
    "        dones = data[\"dones\"]\n",
    "        # clean the memory from the data file\n",
    "        del data\n",
    "        # return the transitions\n",
    "        return Transition(states=states, actions=actions, next_states=next_states, rewards=rewards, dones=dones)\n",
    "\n",
    "# Itereates over the dataset subset in the known manner.\n",
    "class PartialDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data.states)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # select stack of states\n",
    "        states = self.data.states[idx]\n",
    "        # select followup action, next_state, reward and done flag\n",
    "        action = self.data.actions[idx]\n",
    "        next_state = self.data.next_states[idx]\n",
    "        reward = self.data.rewards[idx]\n",
    "        done = self.data.dones[idx]\n",
    "\n",
    "        return states, action, next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vViEbSRZj_4x"
   },
   "source": [
    "# Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfbjibDkCqEG"
   },
   "outputs": [],
   "source": [
    "img_stack = 4\n",
    "show_hud = True\n",
    "batchsize = 128\n",
    "epochs = 100\n",
    "use_colab_autodownload = False\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: \" + str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcUOi1VRmPTm"
   },
   "source": [
    "# BCQ Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbVFFMoSmSpO"
   },
   "outputs": [],
   "source": [
    "# Defining a Q-Network for predicting and evaluating the next action given a state\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, img_stack, n_units_out):\n",
    "        super(QNet, self).__init__()\n",
    "        self.n_units_in = img_stack\n",
    "        self.n_units_out = n_units_out\n",
    "        # =========== YOUR CHANGES =============\n",
    "        # ######################################\n",
    "        # Use the network architecture of your choice\n",
    "        # to extract the features and compute the\n",
    "        # q values, and logits of the policy.\n",
    "        # [HINT]: Don't use Softmax, LogSoftmax or similar\n",
    "        # non parametric layers, since ONNX does not support this.\n",
    "        # ######################################\n",
    "        # 1) CNN feature extraction\n",
    "        # 2) q values output head\n",
    "        # 3) policy output head\n",
    "\n",
    "    @staticmethod\n",
    "    def _weights_init(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
    "            if self.bias is not None:\n",
    "                fan_in, _ = init._calculate_fan_in_and_fan_out(m.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                init.uniform_(m.bias, -bound, bound)\n",
    "\n",
    "        elif instance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # =========== YOUR CHANGES =============\n",
    "        # ######################################\n",
    "        # Use the network architecture of your choice\n",
    "        # to extract the features and compute the\n",
    "        # q values, and logits of the policy\n",
    "        # ######################################\n",
    "        # 1) extract features with an CNN\n",
    "        # 2) compute q values\n",
    "        # 3) compute policy logits\n",
    "\n",
    "        # returns q-function, log action probability and action logits\n",
    "        return q_vals, pi_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5cQGAkGXbsg"
   },
   "outputs": [],
   "source": [
    "# Agent for the Discrete Batch-Constrained deep Q-Learning (BCQ) https://github.com/sfujim/BCQ/tree/master/discrete_BCQ\n",
    "class ORLAgent(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        logger,\n",
    "        img_stack, # image stack\n",
    "        threshold=0.3, # threshold to bias away actions\n",
    "        eval_eps=0.001, # action sampling epsilon\n",
    "        # =========== YOUR CHANGES =============\n",
    "        # ######################################\n",
    "        # Find proper hyperparameters for \n",
    "        # discount factor, tau and adam optimizer\n",
    "        # ######################################\n",
    "        discount=..., # discount factor for Q-value computation\n",
    "        lambda_=..., # regularization parameter\n",
    "        tau=..., # parameter for exponential moving average of Q parameter updates\n",
    "        optimizer=\"Adam\", # optimizer \n",
    "        optimizer_parameters={ # hyperparameters for optimizer\n",
    "\t\t\t\"lr\": ...,\n",
    "\t\t\t\"eps\": ...\n",
    "\t\t}\n",
    "\t):\n",
    "        self.logger = logger\n",
    "        self.num_actions = len(action_mapping)\n",
    "\n",
    "        # Create Q-network\n",
    "        self.Q = QNet(img_stack, self.num_actions).to(device)\n",
    "        # Create a copy of the current Q-Network as the target network\n",
    "        self.Q_target = copy.deepcopy(self.Q)\n",
    "        # Initialize optimizer\n",
    "        self.Q_optimizer = getattr(torch.optim, optimizer)(self.Q.parameters(), **optimizer_parameters)\n",
    "        # Q value discount factor\n",
    "        self.discount = discount\n",
    "\n",
    "        # Target update rule exponential move average hyperparameter\n",
    "        self.tau = tau\n",
    "        # Evaluation hyperparameter for action selection\n",
    "        self.eval_eps = eval_eps\n",
    "        # Threshold for biasing unlikely actions away\n",
    "        self.threshold = threshold\n",
    "        # Regularization hyperparameter\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "        num_trainable_params = sum(p.numel() for p in self.Q.parameters() if p.requires_grad)\n",
    "        print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
    "\n",
    "    def _get_action_idx(self, state):\n",
    "        # =========== YOUR CHANGES =============\n",
    "        # ######################################\n",
    "        # Get the action index based on the state provided\n",
    "        # using the Q-Network\n",
    "        # ######################################\n",
    "        # 1) get current state q values and policy logits\n",
    "        # 2) retriev probabilities through softmax\n",
    "        # 3) create action mask using the threshold\n",
    "        # 4) compute the next action index with the weighted q values and mask\n",
    "        return action_idx\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Select action according to policy with probability (1-eps)\n",
    "        # otherwise, select random action\n",
    "        if np.random.uniform(0,1) > self.eval_eps:\n",
    "            with torch.no_grad():\n",
    "                next_action_idx = self._get_action_idx(state)\n",
    "                return action_mapping[next_action_idx]\n",
    "        else:\n",
    "            # Randomly select an action uniformly\n",
    "            next_action_idx = np.random.randint(self.num_actions)\n",
    "            return action_mapping[next_action_idx]\n",
    "\n",
    "    @torch.enable_grad()\n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        # Compute the target Q value\n",
    "        with torch.no_grad():\n",
    "            action_idx = self._get_action_idx(next_state)\n",
    "            # Get target q-function\n",
    "            q, _ = self.Q_target(next_state)\n",
    "            # =========== YOUR CHANGES =============\n",
    "            # ######################################\n",
    "            # Calculate the target q values to \n",
    "            # update the Q-Network\n",
    "            # ######################################\n",
    "            # 1) compute the target Q-value\n",
    "            target_Q = ...\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q, logits = self.Q(state)\n",
    "        # Gather actions along dimension\n",
    "        current_Q = current_Q.gather(1, action)\n",
    "        # Get log probabilities from logits\n",
    "        log_probs = self.log_softmax(logits)\n",
    "\n",
    "        # =========== YOUR CHANGES =============\n",
    "        # ######################################\n",
    "        # Compute the loss based on the q values,\n",
    "        # the policy constrain and an optional \n",
    "        # regularization.\n",
    "        # ######################################\n",
    "        # 1) compute Q loss using the smoothed L1 loss\n",
    "        # 2) compute policy loss via the negative log-likelihood between log probabilites and demonstration actions\n",
    "        # 3) regularize based on logits \n",
    "        # 4) compute total loss\n",
    "        # 5) take a backward step on the Q function\n",
    "        # 6) update target network by polyak by iterating over the Q-Network and target Q-Network parameters\n",
    "        #    use tau parameter to compute the exponential moving average Q-Network parameters and update the target network\n",
    "\n",
    "        # Return loss\n",
    "        return loss.cpu().item()\n",
    "\n",
    "    def mode(self, mode):\n",
    "        # switch networks between evaluation and train mode\n",
    "        if mode == 'train':\n",
    "            self.Q.train()\n",
    "        else:\n",
    "            self.Q.eval()\n",
    "\n",
    "    def save(self, param_file, sample):\n",
    "        torch.save(self.Q.state_dict(), param_file)\n",
    "        save_as_onnx(self.Q, sample, f'{param_file}_onnx')\n",
    "        # download param file\n",
    "        if use_colab_autodownload: download_colab_model(param_file)\n",
    "\n",
    "    def load_param(self, param_file):\n",
    "        self.Q.load_state_dict(torch.load(param_file, map_location=\"cpu\"))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjNFcS2jmVL5"
   },
   "source": [
    "# Define Training and Validation Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtVHY28YmYp-"
   },
   "outputs": [],
   "source": [
    "def train_epoch(agent, train_set, logger, epoch, pbar):\n",
    "    # Switch to train mode\n",
    "    agent.mode('train')\n",
    "    # Initialize helpers variables\n",
    "    ts_len = len(train_set)\n",
    "    running_loss = None\n",
    "    alpha = 0.3\n",
    "    # =========== [OPTIONAL] CHANGES =============\n",
    "    # ############################################\n",
    "    # [Hint]: Accessing the file system is slow and you can\n",
    "    # reshape your data / load multiple files in to speed up \n",
    "    # training.\n",
    "    # ############################################\n",
    "    # Iterate over the list of demonstration files\n",
    "    for i, idx in enumerate(BatchSampler(SubsetRandomSampler(range(ts_len)), 1, False)):\n",
    "        # Load the selected index from the filesystem\n",
    "        data = train_set.load(idx[0])\n",
    "        # Create dataset from loaded data sub-set\n",
    "        partial = PartialDataset(data)\n",
    "        # Create dataloader\n",
    "        loader = DataLoader(partial, batch_size=batchsize, num_workers=1, shuffle=True, drop_last=False, pin_memory=True)\n",
    "        l_len = len(loader)\n",
    "        # Iterate over parial dataset\n",
    "        for j, (s, a, s_, r, d) in enumerate(loader):\n",
    "            # Adjust types, shape and push to device\n",
    "            s = s.float().to(device)\n",
    "            a = a.long().unsqueeze(1).to(device)\n",
    "            s_ = s_.float().to(device)\n",
    "            r = r.float().unsqueeze(1).to(device)\n",
    "            d = d.float().unsqueeze(1).to(device)\n",
    "            # Train the respective agent\n",
    "            loss = agent.train(s, a, s_, r, d)\n",
    "            # Update running average loss\n",
    "            running_loss = loss if running_loss is None else loss * alpha + (1 - alpha) * running_loss\n",
    "            # Update info in the progress bar\n",
    "            pbar.set_postfix_str(\"Epoch: %03d/%03d Partial: %03d/%03d Idx: %03d/%03d Loss: %.4f\" % (epoch+1, epochs, i+1, ts_len, j+1, l_len, running_loss))\n",
    "    return running_loss, s  # s serves as sample input for saving the model in ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_Zo24Kb9jnh"
   },
   "source": [
    "# Evaluate the agent in the real environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nZQ-LLp9jTB"
   },
   "outputs": [],
   "source": [
    "class Env():\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing \n",
    "    \"\"\"\n",
    "    def __init__(self, img_stack, show_hud=True, record_video=True, seed=None):\n",
    "        self.record_video=record_video\n",
    "        # Create gym environment\n",
    "        self.gym_env = gym.make('CarRacing-v0')\n",
    "        if seed:\n",
    "            print(f\"Environment seed: {seed}\")\n",
    "            self.gym_env.seed(seed)\n",
    "            self.gym_env.action_space.seed(seed)\n",
    "        self.env, self.video_dir = self.wrap_env(self.gym_env)\n",
    "        self.action_space = self.env.action_space\n",
    "        self.img_stack = img_stack\n",
    "        self.show_hud = show_hud\n",
    "\n",
    "    def reset(self, raw_state=False):\n",
    "        self.env, self.video_dir = self.wrap_env(self.gym_env)\n",
    "        self.rewards = []\n",
    "        img_rgb = self.env.reset()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        if not self.show_hud:\n",
    "            img_gray = hide_hud(img_gray)\n",
    "        self.stack = [img_gray] * self.img_stack\n",
    "        if raw_state:\n",
    "            return np.array(self.stack), np.array(img_rgb)\n",
    "        else:\n",
    "            return np.array(self.stack)\n",
    "\n",
    "    def step(self, action, raw_state=False):        \n",
    "        # for i in range(self.img_stack):\n",
    "        img_rgb, reward, done, _ = self.env.step(action)            \n",
    "        # accumulate reward\n",
    "        self.rewards.append(reward)            \n",
    "        # if no reward recently, end the episode\n",
    "        die = True if np.mean(self.rewards[-np.minimum(100, len(self.rewards)):]) <= -1 else False\n",
    "        if done or die:\n",
    "            self.env.close()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        if not self.show_hud:\n",
    "            img_gray = hide_hud(img_gray)\n",
    "        # add to frame stack  \n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == self.img_stack\n",
    "        # --\n",
    "        if raw_state:\n",
    "            return np.array(self.stack), np.sum(self.rewards[-1]), done, die, img_rgb\n",
    "        else:\n",
    "            return np.array(self.stack), np.sum(self.rewards[-1]), done, die\n",
    "\n",
    "    def render(self, *arg):\n",
    "        return self.env.render(*arg)\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "  \n",
    "    def wrap_env(self, env):\n",
    "        \"\"\"\n",
    "        Wrapper for recording video of the environment.\n",
    "        \"\"\"\n",
    "        outdir = f\"./videos/\"\n",
    "        if os.path.exists(outdir):\n",
    "            shutil.rmtree(outdir)\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        if self.record_video:\n",
    "            env = Monitor(env, outdir, force=True)\n",
    "        return env, outdir\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_episode(agent, n_runs=1, record_video=False, logger=None, pbar=None):\n",
    "    agent.mode('eval')\n",
    "    score_avg = None\n",
    "    alpha = 0.3\n",
    "    env_seeds = [np.random.randint(1e7) for _ in range(n_runs)]\n",
    "    for i in range(n_runs):\n",
    "        # Create new environment object\n",
    "        env = Env(img_stack=img_stack, record_video=record_video, seed=env_seeds[i])\n",
    "        state = env.reset()\n",
    "        done_or_die = False\n",
    "        score = 0\n",
    "        while not done_or_die:\n",
    "            t_state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            action = agent.select_action(t_state)\n",
    "            state, reward, done, die = env.step(action)\n",
    "            score += reward\n",
    "            if pbar:\n",
    "                pbar.set_postfix_str(\"Env Evaluation - Run {:03d} Score: {:.2f}\".format(i+1, score))\n",
    "            if done or die:\n",
    "                done_or_die = True\n",
    "            sleep(0.001)\n",
    "        env.close()\n",
    "        score_avg = score if score_avg is None else score * alpha + (1 - alpha) * score_avg\n",
    "        print(f\"Evaluation run {i} completed!\")\n",
    "    return score_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tv8P8s4rnOZM"
   },
   "source": [
    "# Train your agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUTL4qrZOQtw"
   },
   "outputs": [],
   "source": [
    "# Specify the google drive mount here if you want to store logs and weights there (and set it up earlier)\n",
    "logger = Logger(\"logdir\")\n",
    "print(\"Saving state to {}\".format(logger.basepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzfqdEIgvnnJ"
   },
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "train_set = Demonstration(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_F4YgTfxNrI"
   },
   "outputs": [],
   "source": [
    "# Create new agent\n",
    "agent = ORLAgent(logger, img_stack=img_stack)\n",
    "\n",
    "# Optionally load existing parameter file\n",
    "#param_file = 'logdir/2021-03-18T16-44-29/params.pkl'\n",
    "#agent.load_param(param_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pM7ylckLnRnP"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "epoch_iter = range(epochs)\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with tqdm(epoch_iter) as pbar:\n",
    "    for i_ep in pbar:\n",
    "        print(f\"Starting training epoch {i_ep+1}/{epochs}\")\n",
    "        # plot current training state\n",
    "        if i_ep > 0:\n",
    "            with out:\n",
    "                display.clear_output(wait=True)\n",
    "                plot_metrics(logger)\n",
    "        # train\n",
    "        train_loss, sample = train_epoch(agent, train_set, logger, i_ep, pbar)\n",
    "        logger.log(\"training_loss\", train_loss)\n",
    "        # =========== [OPTIONAL] CHANGES =============\n",
    "        # ############################################\n",
    "        # Go full Offline RL if you feel up to it. :)\n",
    "        # [Hint]: Evaluate in the environment - strictly speaking this is not allowed in pure Offline RL!!!\n",
    "        # But we ease the task a bit and avoid that you are flying blind all the time.\n",
    "        # Otherwise you would be only allowed to test once you submit to the challenge server.\n",
    "        # If you are really looking for a challenge feel free to remove this line and make a train/eval data split from the demonstrations.\n",
    "        # ############################################\n",
    "        score = run_episode(agent, logger=logger, pbar=pbar)\n",
    "        logger.log(\"env_score\", score)\n",
    "        # store logs\n",
    "        logger.dump()\n",
    "        # store weights\n",
    "        print(\"Saving state to {}\".format(logger.basepath))\n",
    "        save_file_path = f'{logger.param_file}_%03d' % i_ep\n",
    "        agent.save(save_file_path, sample)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(\"Saved state to {}\".format(logger.basepath))\n",
    "print(\"Trainable Parameters: {}\".format(num_trainable_params))\n",
    "print(\"[%03d] Training Loss: %.4f\" % (i_ep + 1, train_loss))\n",
    "plot_metrics(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVrASW_Hy1lo"
   },
   "source": [
    "# Visualize Agent Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkjTFDi3y72N"
   },
   "source": [
    "### Put the agent into a real environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0JaMOvxzBio"
   },
   "source": [
    "Let's see how the agent is doing in the real environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrqzgG-bzXws"
   },
   "outputs": [],
   "source": [
    "# select agent you want to evaluate\n",
    "agent = ORLAgent(logger, img_stack=img_stack)\n",
    "\n",
    "# load parameter\n",
    "#param_file = 'logdir/2021-03-29T09-24-10/params.pkl_009'\n",
    "#agent.load_param(param_file)\n",
    "\n",
    "# run episode with recording and show video\n",
    "run_episode(agent, n_runs=1, record_video=True)\n",
    "show_video(env.video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aX2Le-9Eg0y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "03-OfflineRL.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}